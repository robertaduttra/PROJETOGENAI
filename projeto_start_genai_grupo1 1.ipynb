{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings \n",
    "from langchain.vectorstores.pgvector import PGVector \n",
    "from langchain_community.document_loaders import DirectoryLoader \n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.vectorstores import FAISS\n",
    "from sqlalchemy import create_engine, text\n",
    "from openai import AzureOpenAI \n",
    "from dotenv import load_dotenv\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando variáveis que serão usadas durante o processo de RAG e Chat\n",
    "# Nome dos modelos no endpoint\n",
    "embedding_model_name = 'embeddings-start'\n",
    "openai_model_name = 'projeto-start'\n",
    "\n",
    "# Importação das variáveis de ambiente que serão usadas em diversas funções (endpoint e chave)\n",
    "load_dotenv()\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_API_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Local com FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando o client de embeddings\n",
    "encoder = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embedding_model_name,\n",
    "    openai_api_version=\"2023-05-15\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lendo a base de conhecimento\n",
    "loader = DirectoryLoader('../Documentação/', glob='**/**/*.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "# Dividindo documento em chunks e gerando embeddings\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    separators = [\"\\n\\n\"]\n",
    ")\n",
    "\n",
    "split_doc = r_splitter.split_documents(docs)\n",
    "\n",
    "vector = FAISS.from_documents(split_doc, encoder)\n",
    "\n",
    "# Salvando base local\n",
    "vector.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregando FAISS db localmente\n",
    "local_db = FAISS.load_local('faiss_index', encoder, allow_dangerous_deserialization= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG com PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A coleção com o Nome 'documentation' não foi encontrada. Criando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VITORFER\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\vectorstores\\pgvector.py:328: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Inicializando chave de conexão com o banco\n",
    "CONNECTION_STRING = os.getenv('CONNECTION_STRING')\n",
    "COLLECTION_NAME = 'documentation'\n",
    "\n",
    "# Iniciando o client de embeddings\n",
    "encoder = AzureOpenAIEmbeddings(\n",
    "    azure_deployment= embedding_model_name,\n",
    "    openai_api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "\n",
    "if COLLECTION_NAME != '':\n",
    "        \n",
    "    # Crie uma engine de conexão com o banco de dados\n",
    "    engine = create_engine(CONNECTION_STRING)\n",
    "            \n",
    "    # Consultas SQL\n",
    "    check_collection_query = text(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM langchain_pg_collection\n",
    "        WHERE name = :collection_name\n",
    "        \"\"\")     \n",
    "    \n",
    "\n",
    "    # Consulta para excluir dados associados à coleção\n",
    "    delete_collection_query = text(\"\"\"\n",
    "        DELETE FROM langchain_pg_collection\n",
    "        WHERE name = :collection_name        \n",
    "        \"\"\")  \n",
    "    \n",
    "    \n",
    "    # Execute a consulta e verifique o resultado\n",
    "    with engine.connect() as connection:\n",
    "        \n",
    "        transaction = connection.begin()\n",
    "        \n",
    "        try:\n",
    "            result = connection.execute(check_collection_query, {'collection_name': COLLECTION_NAME})\n",
    "            count = result.scalar()  # Obtém o valor único da consulta\n",
    "            \n",
    "            if count > 0:\n",
    "                print(f\"A coleção com o Nome '{COLLECTION_NAME}' existe. Sobrescrevendo....\")\n",
    "                \n",
    "                connection.execute(delete_collection_query, {'collection_name': COLLECTION_NAME})\n",
    "                \n",
    "                transaction.commit() \n",
    "                \n",
    "                #Carregar o conteúdo do arquivo de texto\n",
    "                loader = TextLoader('../Documentação/documentation.txt', encoding='utf-8')\n",
    "                documents = loader.load() \n",
    "\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, separators=['\\n\\n'])\n",
    "\n",
    "                texts = text_splitter.split_documents(documents) \n",
    "                \n",
    "                # Gera e armazena embeddings no banco de dados usando a classe PGVector\n",
    "                db = PGVector.from_documents( \n",
    "\n",
    "                embedding=encoder,\n",
    "                documents=texts,\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                connection_string=CONNECTION_STRING\n",
    "                )           \n",
    "            else:\n",
    "                print(f\"A coleção com o Nome '{COLLECTION_NAME}' não foi encontrada. Criando...\")\n",
    "                \n",
    "                #Carregar o conteúdo do arquivo de texto\n",
    "                loader = TextLoader('../Documentação/documentation.txt', encoding='utf-8')\n",
    "                documents = loader.load() \n",
    "\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, separators=['\\n\\n'])\n",
    "\n",
    "                texts = text_splitter.split_documents(documents) \n",
    "                \n",
    "                # Gera e armazena embeddings no banco de dados usando a classe PGVector\n",
    "                db = PGVector.from_documents( \n",
    "                                \n",
    "                embedding=encoder,\n",
    "                documents=texts,\n",
    "                collection_name=COLLECTION_NAME, \n",
    "                connection_string=CONNECTION_STRING\n",
    "                )\n",
    "                     \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao executar a consulta: {e}\")\n",
    "            \n",
    "    # Feche a conexão\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando prompt com as informações da base de conhecimento\n",
    "pergunta = \"Como me identifico?\"\n",
    "\n",
    "query_embedding = encoder.embed_query(pergunta)\n",
    "\n",
    "retrieved_docs = db.similarity_search_by_vector(query_embedding, kwargs= 3)  #Para usar o FAISS ao inves do Postgres, basta mudar a variável db para local_db\n",
    "\n",
    "# Ajuste para melhor leitura no prompt\n",
    "contexto = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Utilize o contexto a seguir para responder a pergunta em uma única sentença. Só responda caso a resposta seja encontrada no \"contexto\".\n",
    "    pergunta : {pergunta}\n",
    "    contexto : {contexto}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinto muito, mas não encontrei uma pergunta clara sobre o funcionamento da API do Spotify. Você tem alguma pergunta relacionada à API do Spotify?\n"
     ]
    }
   ],
   "source": [
    "# Iniciando client de resposta da Azure OpenAI\n",
    "client = AzureOpenAI(\n",
    "    api_key = AZURE_OPENAI_API_KEY,  \n",
    "    api_version = \"2024-02-01\",\n",
    "    azure_endpoint = AZURE_OPENAI_API_ENDPOINT,\n",
    ")\n",
    "\n",
    "\n",
    "# Enviando a pergunta e recebendo a resposta\n",
    "completion = client.chat.completions.create(\n",
    "    model=openai_model_name,  # e.g. gpt-35-instant,\n",
    "    temperature=0.8,    \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Você é um assistente que responde de maneira educada somente perguntas sobre o funcionamento da API do Spotify. Caso a pergunta não tenha relação com a API do Spotify, peça desculpas, não responda e peça perguntas relacionadas à API do Spotify.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
